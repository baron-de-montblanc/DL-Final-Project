{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning for Jet Tagging in Particle Physics: GNN\n",
    "\n",
    "This is the second accompanying notebook for our final project for the CSCI 2470: Deep Learning course. Here, we will present appropriate visualizations of our input data, build and train our models, and present appropriate visualizations of the outputs and results.\n",
    "\n",
    "*Authors: Jade Ducharme, Egor Serebriakov, Aditya Singh, Anthony Wong*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: Transfer Learning via GNN\n",
    "\n",
    "The current state-of-the-art jet tagging model uses a Graph Neural Network architecture. Our second goal is then to build a Teacher and a Student GNN and implement transfer learning similarly to what we did with the FCCN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchmetrics\n",
    "import torch.optim as optim\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from preprocess import *\n",
    "from helper import *\n",
    "\n",
    "sns.set_theme()\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the first notebook, we will start by loading in the data and preprocessing. Note that since we are now working with *graphs*, the preprocessing steps will differ significantly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Constituent-level data ---------------- \n",
      "\n",
      "Data shape [input_size, num_features, num_constituents]: (10000, 4, 80) \n",
      "\n",
      "Feature names: ['fjet_clus_E' 'fjet_clus_eta' 'fjet_clus_phi' 'fjet_clus_pt']\n",
      "Feature names (human-readable): ['constituent energy', 'constituent pseudo-rapidity', 'constituent azimuthal angle', 'constituent transverse momentum']\n"
     ]
    }
   ],
   "source": [
    "cons_data, cons_labels, cons_weights, cons_features = get_data(\"./data/reduced_atlas_dataset.h5\", attribute=\"constituents\")\n",
    "\n",
    "print(\"---------- Constituent-level data ----------------\", \"\\n\")\n",
    "print(\"Data shape [input_size, num_features, num_constituents]:\", cons_data.shape, \"\\n\")\n",
    "print(\"Feature names:\", cons_features)\n",
    "print(\"Feature names (human-readable):\", [human_feature(f) for f in cons_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will preprocess this using the same steps as for the FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_cons_data, pre_cons_features = constituent_preprocess(cons_data, cons_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement our GNN, we then need to form graphs using our data. We will do so using the k-nearest-neighbors algorithm, and we will try to run on GPU to accelerate the graph making process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 11.11it/s]\n"
     ]
    }
   ],
   "source": [
    "from preprocess import prepare_graphs\n",
    "\n",
    "# Check for CUDA, then default to CPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "print(\"Device:\", device)\n",
    "\n",
    "graphs = prepare_graphs(pre_cons_data, cons_labels, k=16, weights=cons_weights, device=device, batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize\n",
    "\n",
    "We can visualize the graphs we just created!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize import visualize_graph\n",
    "\n",
    "# visualize_graph(graphs[100], x_axis='fjet_clus_phi', y_axis='fjet_clus_eta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the Teacher GNN\n",
    "\n",
    "Let's build the Teacher GNN and train it on the high-resolution data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "E 05 -- Train loss: 0.6873 -- Train acc: 0.5057 -- Val loss: 0.6939 -- Val acc: 0.4787 -- t elapsed: 0.11 min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,num_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# training\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     loss, acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch_gnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteacher_gnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     loss_list\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     31\u001b[0m     acc_list\u001b[38;5;241m.\u001b[39mappend(acc)\n",
      "File \u001b[0;32m~/Brown/CSCI/Deep_Learning/DL-Final-Project/model.py:264\u001b[0m, in \u001b[0;36mtrain_one_epoch_gnn\u001b[0;34m(model, device, train_loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m    261\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[1;32m    262\u001b[0m weighted_loss \u001b[38;5;241m=\u001b[39m (loss \u001b[38;5;241m*\u001b[39m weights)\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# Apply weights and average\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m \u001b[43mweighted_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    267\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m weighted_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/final_project/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/final_project/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from model import TeacherGNN\n",
    "from preprocess import split_graphs\n",
    "from model import train_one_epoch_gnn, test_gnn\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device:\",device)\n",
    "\n",
    "# prepare data\n",
    "train_dataset, val_dataset, test_dataset = split_graphs(graphs, 0.7, 0.15)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=384, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=384, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=384, shuffle=True)\n",
    "\n",
    "# initialize model\n",
    "teacher_gnn = TeacherGNN().to(device)\n",
    "optimizer = optim.AdamW(teacher_gnn.parameters(), lr=3e-4, weight_decay=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "loss_list, acc_list = [], []\n",
    "val_loss_list, val_acc_list = [], []\n",
    "\n",
    "# start training\n",
    "t0 = time.time()\n",
    "num_epochs = 30\n",
    "for e in range(1,num_epochs+1):\n",
    "\n",
    "    # training\n",
    "    loss, acc = train_one_epoch_gnn(teacher_gnn, device, train_loader, optimizer, criterion)\n",
    "    loss_list.append(loss)\n",
    "    acc_list.append(acc)\n",
    "\n",
    "    # validation\n",
    "    val_loss, val_acc = test_gnn(teacher_gnn, device, val_loader, criterion)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_acc_list.append(val_acc)\n",
    "    \n",
    "    if e%5 == 0:\n",
    "        print(f\"E {e:02d} -- Train loss: {loss:.4f} -- Train acc: {acc:.4f} -- \"\\\n",
    "                + f\"Val loss: {val_loss:.4f} -- Val acc: {val_acc:.4f} -- \"\\\n",
    "              + f\"t elapsed: {time_elapsed(t0, time.time())} min\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize import plot_loss_and_accuracy\n",
    "\n",
    "plot_loss_and_accuracy(loss_list, val_loss_list, acc_list, val_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize import plot_confusion_matrices\n",
    "\n",
    "true = []\n",
    "pred = []\n",
    "for d in test_dataset:\n",
    "    l = d.y.item()\n",
    "    true.append(int(l))\n",
    "    p = np.argmax(teacher_gnn(d.to(device)).cpu().detach().numpy())\n",
    "    pred.append(p)\n",
    "\n",
    "plot_confusion_matrices(true, pred, \"Teacher GNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
